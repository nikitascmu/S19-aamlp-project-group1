#--- replacing '99' in hourlocal to NA
rct_train_subset = rct_train_subset %>%
mutate(HOURLOCAL = ifelse(rct_train_subset$HOURLOCAL==99, NA, rct_train_subset$HOURLOCAL))
#--- replacing '99' in minlocal to NA
rct_train_subset = rct_train_subset %>%
mutate(MINLOCAL = ifelse(rct_train_subset$MINLOCAL==99, NA, rct_train_subset$MINLOCAL))
#--- imputing train set missing data
#--- default is m=5 and maxit=5
m_rct_train = mice(rct_train_subset %>%
select(-outcome) %>%
mutate_if(is.character, as.factor), print=FALSE)
#--- given that imputation takes a while, I used the first imputed set
imputedtrain = complete(m_rct_train,1)
#--- adding missingness indicators for hourlocal, minlocal, ratrial, rasp3, and rhep24
missing = apply(rct_train_subset, FUN = (function(x) any(is.na(x))),MARGIN = 2)
train_miss = is.na(rct_train_subset) %>% as_tibble() %>% rename_all(~ paste0(., "_missing"))
train_miss = train_miss[,missing]
imputedtrain = cbind(imputedtrain, train_miss)
#--- adding back the outcome column to the train set
imputedtrain$outcome = rct_train_subset$outcome
imputedtrain = imputedtrain %>% mutate_if(is.logical, as.factor)
#--- summarizing the imputed train set
summary(imputedtrain)
#logistic regression, default parameters
logit_modelFit <- glm(as.factor(outcome) ~ ., data=imputedtrain, family = binomial("logit"))
#random forest, default parameters
forest_modelFit = randomForest(formula = as.factor(outcome) ~ ., data=imputedtrain)
#boosting, default parameters
aforest_modelFit = ada(as.factor(outcome) ~ ., data=imputedtrain)
#cross validation to determine best parameters for gradient boosting
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
#gradient boosting
gbmFit_cv <- train(as.factor(outcome) ~ ., data = imputedtrain,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
#--- choosing which variables to include in model
colnames_to_subset = c("RDELAY", "RCONSC", "SEX", "AGE", "RSLEEP", "RATRIAL", "RCT",
"RVISINF", "RHEP24", "RASP3", "RSBP", "RDEF1", "RDEF2", "RDEF3",
"RDEF4", "RDEF5", "RDEF6", "RDEF7", "RDEF8", "STYPE", "RDATE",
"HOURLOCAL", "MINLOCAL", "DAYLOCAL", "RXASP", "RXHEP", "outcome")
#--- subsetting rct train set based on above chosen variables
rct_subset = rct_data_random %>% select(one_of(colnames_to_subset))
rct_subset = rct_subset %>% mutate_if(is.character, as.factor)
#--- column names of rct train subsetted data
colnames(rct_subset)
#--- summarizing my train data subset
summary(rct_subset)
#--- replacing '99' in hourlocal to NA
rct_subset = rct_subset %>%
mutate(HOURLOCAL = ifelse(rct_subset$HOURLOCAL==99, NA, rct_subset$HOURLOCAL))
#--- replacing '99' in minlocal to NA
rct_subset = rct_subset %>%
mutate(MINLOCAL = ifelse(rct_subset$MINLOCAL==99, NA, rct_subset$MINLOCAL))
#--- imputing train set missing data
#--- default is m=5 and maxit=5
m_rct_data = mice(rct_subset %>%
select(-outcome) %>%
mutate_if(is.character, as.factor), print=FALSE)
#--- imputing train set missing data
#--- default is m=5 and maxit=5
m_rct_data = mice(rct_subset %>%
select(-outcome) %>%
mutate_if(is.character, as.factor), print=FALSE)
#--- given that imputation takes a while, I used the first imputed set
imputeddata = complete(m_rct_train,1
#--- adding missingness indicators for hourlocal,
inlocal, ratrial, rasp3, and rhep24
#--- given that imputation takes a while, I used the first imputed set
imputeddata = complete(m_rct_data,1)
imputedtrain = imputeddata[1:(nrow(imputeddata)*0.50),]
imputedtest = imputeddata[-(1:(nrow(imputeddata)*0.50)),]
#--- adding missingness indicators for hourlocal, minlocal, ratrial, rasp3, and rhep24
missing = apply(rct_subset, FUN = (function(x) any(is.na(x))),MARGIN = 2)
miss = is.na(rct_subset) %>% as_tibble() %>% rename_all(~ paste0(., "_missing"))
miss = miss[,missing]
imputeddata = cbind(imputeddata, miss)
imputedtrain = imputeddata[1:(nrow(imputeddata)*0.50),]
imputedtest = imputeddata[-(1:(nrow(imputeddata)*0.50)),]
imputeddata$outcome = rct_subset$outcome
imputedtrain = imputeddata[1:(nrow(imputeddata)*0.50),]
imputedtest = imputeddata[-(1:(nrow(imputeddata)*0.50)),]
imputeddata = imputeddata %>% mutate_if(is.logical, as.factor)
imputedtrain = imputeddata[1:(nrow(imputeddata)*0.50),]
imputedtest = imputeddata[-(1:(nrow(imputeddata)*0.50)),]
#--- summarizing the imputed train set
summary(imputedtrain)
#logistic regression, default parameters
logit_modelFit <- glm(as.factor(outcome) ~ ., data=imputedtrain, family = binomial("logit"))
logit_predictions = predict(logit_modelFit, imputedtest, type = "response") %>% as_tibble()
columns_for_ROC_logit = imputedtest %>% select(outcome) %>% bind_cols(logit_predictions)
logit_pred = prediction(predictions=columns_for_ROC_logit$value,labels=columns_for_ROC_logit$outcome)
logit_perf = performance(logit_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_logit = performance(logit_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
logit_plot_data = logit_perf %>% mutate(model_and_auc = paste("Logistic:",round(auc_logit$auc,4)))
View(auc_logit)
#random forest, default parameters
forest_modelFit = randomForest(formula = as.factor(outcome) ~ ., data=imputedtrain)
#random forest, default parameters
forest_modelFit = randomForest(formula = as.factor(outcome) ~ ., data=imputedtrain)
#boosting, default parameters
aforest_modelFit = ada(as.factor(outcome) ~ ., data=imputedtrain)
#cross validation to determine best parameters for gradient boosting
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
#gradient boosting
gbmFit_cv <- train(as.factor(outcome) ~ ., data = imputedtrain,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
logit_predictions = predict(logit_modelFit, imputedtest, type = "response") %>% as_tibble()
forest_predictions = predict(forest_modelFit, imputedtest, type = "prob") %>% as_tibble()
aforest_predictions = predict(aforest_modelFit, imputedtest, type = "probs") %>% as_tibble()
gforest_predictions = predict(gbmFit_cv, imputedtest,
n.trees=gbmFit_cv$bestTune$n.trees,
interaction.depth=gbmFit_cv$bestTune$interaction.depth,
type = "prob")
## logistic
columns_for_ROC_logit = imputedtest %>% select(outcome) %>% bind_cols(logit_predictions)
logit_pred = prediction(predictions=columns_for_ROC_logit$value,labels=columns_for_ROC_logit$outcome)
logit_perf = performance(logit_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_logit = performance(logit_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
logit_plot_data = logit_perf %>% mutate(model_and_auc = paste("Logistic:",round(auc_logit$auc,4)))
## random forest
columns_for_ROC_forest = imputedtest %>% select(outcome) %>% bind_cols(forest_predictions)
forest_pred = prediction(predictions=columns_for_ROC_forest %>% select("1"),labels=columns_for_ROC_forest$outcome)
forest_perf = performance(forest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_forest = performance(forest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
forest_plot_data = forest_perf %>% mutate(model_and_auc = paste("Random Forest:",round(auc_forest$auc,4)))
## boosting
columns_for_ROC_aforest = imputedtest %>% select(outcome) %>% bind_cols(aforest_predictions)
aforest_pred = prediction(predictions=columns_for_ROC_aforest %>% select("V2"),labels=columns_for_ROC_aforest$outcome)
aforest_perf = performance(aforest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_aforest = performance(aforest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
aforest_plot_data = aforest_perf %>% mutate(model_and_auc = paste("Boosting:",round(auc_aforest$auc,4)))
## gradient boosting
columns_for_ROC_gforest = imputedtest %>% select(outcome) %>% bind_cols(gforest_predictions)
gforest_pred = prediction(predictions=columns_for_ROC_gforest %>% select("1"),labels=columns_for_ROC_gforest$outcome)
gforest_perf = performance(gforest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_gforest = performance(gforest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
gforest_plot_data = gforest_perf %>% mutate(model_and_auc = paste("Gradient Boosting:",round(auc_gforest$auc,4)))
roc_plot_data = rbind(logit_plot_data, forest_plot_data, aforest_plot_data, gforest_plot_data)
ggplot(roc_plot_data, aes(x=FPR, y=TPR, colour=model_and_auc)) +
ggtitle("ROC Curve by Model") + theme_bw() + labs(colour="Model and AUC") +
geom_abline(slope=1) + geom_line()
#--- Setting working directory
setwd("C:/Users/nikit/OneDrive/Documents/CMU/Spring 2019/ML Pipeline/Homework 2")
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "glmnet")
loadlibs(libs)
#--- setting global decimal print setting
options(scipen=4)
#-- setting global seed
set.seed(0)
#--- importing the RCT dataset
rct_data = read_csv("http://datashare.is.ed.ac.uk/bitstream/handle/10283/128/IST_corrected.csv")
#--- subsetting the rct data to only include age, gender,
#-- systolic blood pressure and conscious state
rct_selected_cols = rct_data %>%
select("AGE", "SEX", "RSBP", "RCONSC", "RXASP") %>%
rename(Age = AGE, Gender = SEX, Systolic_BP = RSBP,
Conscious_State = RCONSC, Aspirin_Status = RXASP)
#--- mutating columns with type character to factor
rct_selected_cols = rct_selected_cols %>% mutate_if(is.character, as.factor)
#--- descriptive statistics by aspirin status
desc_stats = rct_selected_cols %>% split(.$Aspirin_Status) %>% map(summary, digits=0)
#--- descriptive statistics for in groups of aspirin use
desc_stats$Y
#--- descriptive statistics for in groups of no aspirin use
desc_stats$N
### Data cleaning and preprocessing ###
#--- creating a new variable "outcome" that is 1 if OCCODE==1 and 0 otherwise
rct_data = rct_data %>% mutate(outcome = ifelse(OCCODE==1 | OCCODE==2, 1, 0))
#--- only extracting the year from RCT date so as to reduce the granularity and
#--- be able to control for time
rct_data$RDATE = str_extract(rct_data$RDATE, "[0-9].*")
#--- randomizing rct data and creating train & test sets
rct_data_random = rct_data[sample(1:nrow(rct_data)),]  # permute rows
rct_train = rct_data_random[1:(nrow(rct_data_random)*0.50),]
rct_test = rct_data_random[-(1:(nrow(rct_data_random)*0.50)),]
#--- percent of patients are dead or dependent at 6 months in train set
outcome_train_sum = rct_train %>% group_by(outcome) %>%
summarise(count = n()) %>% mutate(prop = round(count/sum(count),4))
#--- percent of patients are dead or dependent at 6 months in test set
outcome_test_sum = rct_test %>% group_by(outcome) %>%
summarise(count = n()) %>% mutate(prop = round(count/sum(count),4))
#--- choosing which variables to include in model
colnames_to_subset = c("RDELAY", "RCONSC", "SEX", "AGE", "RSLEEP", "RATRIAL", "RCT",
"RVISINF", "RHEP24", "RASP3", "RSBP", "RDEF1", "RDEF2", "RDEF3",
"RDEF4", "RDEF5", "RDEF6", "RDEF7", "RDEF8", "STYPE", "RDATE",
"HOURLOCAL", "MINLOCAL", "DAYLOCAL", "RXASP", "RXHEP", "outcome")
#--- subsetting rct train set based on above chosen variables
rct_train_subset = rct_train %>% select(one_of(colnames_to_subset))
rct_train_subset = rct_train_subset %>% mutate_if(is.character, as.factor)
#--- column names of rct train subsetted data
colnames(rct_train_subset)
#--- summarizing my train data subset
summary(rct_train_subset)
#--- replacing '99' in hourlocal to NA
rct_train_subset = rct_train_subset %>%
mutate(HOURLOCAL = ifelse(rct_train_subset$HOURLOCAL==99, NA, rct_train_subset$HOURLOCAL))
#--- replacing '99' in minlocal to NA
rct_train_subset = rct_train_subset %>%
mutate(MINLOCAL = ifelse(rct_train_subset$MINLOCAL==99, NA, rct_train_subset$MINLOCAL))
#--- imputing train set missing data
#--- default is m=5 and maxit=5
m_rct_train = mice(rct_train_subset %>%
select(-outcome) %>%
mutate_if(is.character, as.factor), print=FALSE)
#--- given that imputation takes a while, I used the first imputed set
imputedtrain = complete(m_rct_train,1)
#--- adding missingness indicators for hourlocal, minlocal, ratrial, rasp3, and rhep24
missing = apply(rct_train_subset, FUN = (function(x) any(is.na(x))),MARGIN = 2)
train_miss = is.na(rct_train_subset) %>% as_tibble() %>% rename_all(~ paste0(., "_missing"))
train_miss = train_miss[,missing]
imputedtrain = cbind(imputedtrain, train_miss)
#--- adding back the outcome column to the train set
imputedtrain$outcome = rct_train_subset$outcome
imputedtrain = imputedtrain %>% mutate_if(is.logical, as.factor)
#--- summarizing the imputed train set
summary(imputedtrain)
#logistic regression, default parameters
logit_modelFit <- glm(as.factor(outcome) ~ ., data=imputedtrain, family = binomial("logit"))
#random forest, default parameters
forest_modelFit = randomForest(formula = as.factor(outcome) ~ ., data=imputedtrain)
#boosting, default parameters
aforest_modelFit = ada(as.factor(outcome) ~ ., data=imputedtrain)
#cross validation to determine best parameters for gradient boosting
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
#gradient boosting
gbmFit_cv <- train(as.factor(outcome) ~ ., data = imputedtrain,
method = "gbm",
trControl = fitControl,
verbose = FALSE)
gbmFit_cv$modelInfo
gbmFit_cv$bestTune
#--- best parameters for gradient boosting based on cross validation
gbmFit_cv$bestTune
#--- subsetting test data to only specific variables as defined earlier
rct_test_subset = rct_test %>% select(one_of(colnames_to_subset))
#--- replacing '99' in hourlocal to NA
rct_test_subset = rct_test_subset %>%
mutate(HOURLOCAL = ifelse(rct_test_subset$HOURLOCAL==99, NA, rct_test_subset$HOURLOCAL))
#--- replacing '99' in minlocal to NA
rct_test_subset = rct_test_subset %>%
mutate(MINLOCAL = ifelse(rct_test_subset$MINLOCAL==99, NA, rct_test_subset$MINLOCAL))
#--- taking all imputed variables except for missingness indicators from the train data
imputedtrain_subset = imputedtrain %>% select(one_of(colnames_to_subset))
#--- joining imputed train set and test set (with missingness)
imptrain_and_test = rbind(imputedtrain_subset, rct_test_subset)
#--- running imputation on this joint set
m_rct_test = mice(imptrain_and_test %>%
select(-outcome) %>%
mutate_if(is.character, as.factor), print=FALSE)
#--- since multiple imputation takes a while, only selected the first imputed set
imputedtest = complete(m_rct_test,1)
#--- extracting only test set rows
imputedtest=imputedtest[-(1:(nrow(imptrain_and_test)*0.50)),]
#--- adding missingness indicators in test set
missing = apply(rct_test_subset, FUN = (function(x) any(is.na(x))),MARGIN = 2)
test_miss = is.na(rct_test_subset) %>% as_tibble() %>% rename_all(~ paste0(., "_missing"))
test_miss = test_miss[,missing]
imputedtest = cbind(imputedtest, test_miss)
#--- adding outcome column in test set
imputedtest$outcome = rct_test_subset$outcome
#--- mutating missingness indicators from logical to factor
imputedtest = imputedtest %>% mutate_if(is.logical, as.factor)
#--- predictions for logistic regression
logit_predictions = predict(logit_modelFit, imputedtest, type = "response") %>% as_tibble()
#--- predictions for random forest
forest_predictions = predict(forest_modelFit, imputedtest, type = "prob") %>% as_tibble()
#--- predictions for boosting
aforest_predictions = predict(aforest_modelFit, imputedtest, type = "probs") %>% as_tibble()
#--- predictions for gradient boosting
gforest_predictions = predict(gbmFit_cv, imputedtest,
n.trees=gbmFit_cv$bestTune$n.trees,
interaction.depth=gbmFit_cv$bestTune$interaction.depth,
type = "prob")
#--- obtaining TPR, FPR, AUC for the logistic regression model
columns_for_ROC_logit = imputedtest %>% select(outcome) %>% bind_cols(logit_predictions)
logit_pred = prediction(predictions=columns_for_ROC_logit$value,labels=columns_for_ROC_logit$outcome)
logit_perf = performance(logit_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_logit = performance(logit_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
logit_plot_data = logit_perf %>% mutate(model_and_auc = paste("Logistic:",round(auc_logit$auc,4)))
#--- obtaining TPR, FPR, AUC for the random forest model
columns_for_ROC_forest = imputedtest %>% select(outcome) %>% bind_cols(forest_predictions)
forest_pred = prediction(predictions=columns_for_ROC_forest %>% select("1"),labels=columns_for_ROC_forest$outcome)
forest_perf = performance(forest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_forest = performance(forest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
forest_plot_data = forest_perf %>% mutate(model_and_auc = paste("Random Forest:",round(auc_forest$auc,4)))
#--- obtaining TPR, FPR, AUC for the boosting model
columns_for_ROC_aforest = imputedtest %>% select(outcome) %>% bind_cols(aforest_predictions)
aforest_pred = prediction(predictions=columns_for_ROC_aforest %>% select("V2"),labels=columns_for_ROC_aforest$outcome)
aforest_perf = performance(aforest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_aforest = performance(aforest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
aforest_plot_data = aforest_perf %>% mutate(model_and_auc = paste("Boosting:",round(auc_aforest$auc,4)))
#--- obtaining TPR, FPR, AUC for the gradient boosting model
columns_for_ROC_gforest = imputedtest %>% select(outcome) %>% bind_cols(gforest_predictions)
gforest_pred = prediction(predictions=columns_for_ROC_gforest %>% select("1"),labels=columns_for_ROC_gforest$outcome)
gforest_perf = performance(gforest_pred, "tpr", "fpr") %>%
(function(.) data.frame(FPR=.@x.values[[1]],
TPR=.@y.values[[1]]) %>% as_tibble())(.)
auc_gforest = performance(gforest_pred, "auc") %>%
(function(.) data.frame(auc=.@y.values[[1]]) %>% as_tibble())(.)
gforest_plot_data = gforest_perf %>% mutate(model_and_auc = paste("Gradient Boosting:",round(auc_gforest$auc,4)))
roc_plot_data = rbind(logit_plot_data, forest_plot_data, aforest_plot_data, gforest_plot_data)
ggplot(roc_plot_data, aes(x=FPR, y=TPR, colour=model_and_auc)) +
ggtitle("ROC Curve by Model") + theme_bw() + labs(colour="Model and AUC") +
geom_abline(slope=1) + geom_line()
#--- extracting importance from the fitted random forest model
importance = forest_modelFit$importance %>% as.data.frame() %>%
mutate(name=as.factor(rownames(.))) %>%
arrange(MeanDecreaseGini) %>%
mutate(name=factor(name, name))
#--- plotting variable importance
ggplot(data=importance) +  geom_bar(aes(x=name, y = MeanDecreaseGini), stat="identity") +
ggtitle("Random Forest Variable Importance") + labs(x="Features") + coord_flip()
directory = "C:/Users/nikit/OneDrive/Documents/CMU/Spring 2019/ML Pipeline/Homework 2"
labevents = fread(paste0(directory, "/labevents.csv"))
labitems = fread(paste0(directory, "/d_labitems.csv"))
#--- only selecting labevents for subject_id 13033
labevents_subid = labevents %>% filter(subject_id==13033)
#--- getting the item ids of all glucose related tests
indices=which(labitems$test_name=="GLUCOSE" & labitems$fluid=="BLOOD" & labitems$category=="CHEMISTRY")
glucose_itemids = labitems$itemid[indices]
#--- restricted lab events for subject id 13033 to only those related to glucose through blood chemistry
glucose_labevents_subid = labevents_subid %>% filter(itemid %in% glucose_itemids)
glucose_labevents_subid = glucose_labevents_subid %>% arrange(charttime) %>%
mutate(charttime=as.character(charttime)) %>%
separate(charttime, c("date", "time"), sep=" ", remove=FALSE)
#--- using "charttime" to get more specific information -- hour.minute
glucose_data = glucose_labevents_subid %>% select(charttime, valuenum, date, time)
glucose_data = glucose_data %>% separate(time, c("hour", "minute", "seconds"), sep=":", remove=FALSE)
glucose_data = glucose_data %>% separate(date, c("year", "month", "day"), sep="-", remove=FALSE)
glucose_data = glucose_data %>% unite(hour_minute, c(hour, minute), sep=".", remove=FALSE)
glucose_data$hour_minute = as.numeric(glucose_data$hour_minute)
#--- plotting glucose data through blood chemistry for subject_id 13033
#--- here, time is measured as hour.minute
ggplot(data=glucose_data, aes(x=hour_minute, y=valuenum)) + geom_point()+
labs(x="Hours in a Day", y="Glucose (mg/dl)") +
ggtitle("Glucose data through blood chemistry for subject ID 13033") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
#--- splitting the data into training and test set
glucose_train = glucose_data %>% filter(year < 3417.5)
glucose_test = glucose_data %>% filter(year > 3417.5)
#--- Function for basis expansion of {sin(kx),cos(kx)} for i = 1 to K
sincos = function(dat, variable="x", period=2*pi, K=10) {
data = dat
for(i in 1:K) {
data[[paste0("sin_",i)]] = sin(data[[variable]]*i*2*pi/period)
} #adding columns for sin_i'th value: ex: sin x, sin 2x, etc.
for(i in 1:K) {
data[[paste0("cos_",i)]] = cos(data[[variable]]*i*2*pi/period)
} #adding k cosine columns
# data$intercept = 1  # not necessary if using with models that use intercepts
data
}
#--- setting period=24 because trying to model daily fluctuations
period = 24
K = 5
#--- basis expansion of train set
glucose_train = sincos(glucose_train, variable="hour_minute", period=period, K=K)
#--- removed certain unneeded columns
colnames_to_remove = c("charttime", "date", "year", "month", "day","time", "hour", "minute", "seconds")
glucose_train = glucose_train %>% select(-one_of(colnames_to_remove))
#--- learning daily trend for the individual on train set using cv.glmnet
glucose_lasso = cv.glmnet(x=glucose_train %>% select(-valuenum, -hour_minute) %>% as.matrix(), y=glucose_train$valuenum, family="gaussian")
#--- here is the coefficient profile with lambda on x axis
plot(glucose_lasso$glmnet.fit, "lambda")
#--- subsetting the test set - removing unneeded columns
glucose_test = glucose_test %>% select(-one_of(colnames_to_remove))
#--- basis expansion of test set
glucose_test = sincos(glucose_test, "hour_minute", period, K)
#--- predicting on test set using earlier fitted lasso
glucose_test$predictions = predict(glucose_lasso,
glucose_test %>% select(-valuenum, -hour_minute) %>% as.matrix(),
s = c(glucose_lasso$lambda.min))
#--- Plotting daily trend in test set plus my predictions
ggplot(data=glucose_test, aes(x=hour_minute,y=valuenum)) + geom_point() +
geom_line(data = glucose_test, aes(x=hour_minute,y=predictions,col="Prediction")) +
geom_line(data = glucose_test, aes(x=hour_minute,y=valuenum, col="Truth")) +
ggtitle("Daily Glucose Trend: Lasso Prediction vs. Truth") +
labs(x="Hour of Day", y="Glucose (mg/dl)", col="Type")
#--- calculating the sum of squares
sse = sum(glucose_test$valuenum - mean(glucose_train$valuenum))^2
#--- calculating the residual sum of squares
rss = sum(glucose_test$valuenum - glucose_test$predictions)^2
#--- calculating the variance
variance = (sse-rss)/sse
#--- calculating the sum of squares
tss = mean(glucose_test$valuenum - mean(glucose_train$valuenum))^2
#--- calculating the residual sum of squares
rss = mean(glucose_test$valuenum - glucose_test$predictions)^2
#--- calculating the variance
variance = (tss-rss)/sse
#--- calculating the sum of squares
tss = mean(glucose_test$valuenum - mean(glucose_train$valuenum))^2
#--- calculating the residual sum of squares
rss = mean(glucose_test$valuenum - glucose_test$predictions)^2
#--- calculating the variance
variance = (tss-rss)/tss
#--- Setting working directory
setwd("C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-projectproposal-group1")
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "glmnet", "readxl")
loadlibs(libs)
#--- setting global decimal print setting
options(scipen=4)
directory = "C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-projectproposal-group1"
prog = fread(paste0(directory, "/program_activity.csv")) %>% as_tibble()
prog %>% group_by(OPIATE_OVERDOSE) %>% summarise(count = length(unique(PERSON_ID)))
dem = fread(paste0(directory, "/demographic.csv")) %>% as_tibble()
presc = fread(paste0(directory, "/opiate_prescription_fills.csv")) %>% as_tibble()
colnames(dem)
head(dem)
head(presc)
View(dem)
prog = fread(paste0(directory, "/program_activity.csv")) %>% as_tibble() %>% mutate_if(is.character, as.factor)
#--- Setting working directory
setwd("C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-project-group1")
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "glmnet", "readxl")
loadlibs(libs)
#--- setting global decimal print setting
options(scipen=4)
#--- loading data
directory = "C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-project-group1"
prog = fread(paste0(directory, "/program_activity.csv")) %>% as_tibble() %>% mutate_if(is.character, as.factor)
dem = fread(paste0(directory, "/demographic.csv")) %>% as_tibble()
presc = fread(paste0(directory, "/opiate_prescription_fills.csv")) %>% as_tibble()
#--- Setting working directory
setwd("C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-project-group1")
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "glmnet", "readxl")
loadlibs(libs)
#--- setting global decimal print setting
options(scipen=4)
#--- loading data
directory = "C:/Users/nikit/OneDrive/Documents/CMU/S19-aamlp-project-group1"
prog = fread(paste0(directory, "/program_activity.csv")) %>% as_tibble() %>% mutate_if(is.character, as.factor)
dem = fread(paste0(directory, "/demographic.csv")) %>% as_tibble() %>% mutate_if(is.character, as.factor)
presc = fread(paste0(directory, "/opiate_prescription_fills.csv")) %>% as_tibble() %>% mutate_if(is.character, as.factor)
summary(dem)
summary(prog)
summary(presc)
