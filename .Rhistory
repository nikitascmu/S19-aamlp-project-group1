model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))
## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')
## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.4)
options(scipen=999)
#performing Oversampling to generate synthetic data
set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
as.data.frame(table(balanced.data$target))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
#instead of training models on all the columns, we are identifying important columns using random forest
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
fit <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
var.imp <- varImp(fit)
plot(var.imp,top=15)
#selecting columns which turns out to be important in random forest
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]
#fitting our first model - logistic regression
fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 5)
model_logistic_cv <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
trControl = fitControl)
#predicted values for testdata:
pred_logistic_cv <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
#test with confusion matrix
table(pred_logistic_cv>0.4,xtest$target)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model_ridge <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
pred_ridge <- model_ridge %>% predict(newx = x.test)
table(pred_ridge>0.2,xtest$target)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
model_rf <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
pred_rf <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
table(pred_rf>0.3,xtest$target)
set.seed(123)
fitControl = trainControl(method="cv", number=10, returnResamp = "all",allowParallel = TRUE)
model_gbm = train(target~., data=balanced.data, method="gbm",distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=5000, .shrinkage=0.1, .interaction.depth=1, .n.minobsinnode=1))
pred_gbm <- predict(model_gbm,xtest,type = 'prob')
table(pred_gbm$'1'>0.2,xtest$target)
stopCluster(cluster)
registerDoSEQ()
model_ada = ada(formula = target ~ .,data=balanced.data ,iter=10)
pred_ada <- predict(model_ada,xtest,type = 'prob')
table(pred_ada[,2]>0.2,xtest$target)
# Neural net
balanced.data2 <- SMOTE(target ~., xtrain, perc.over = 4800, k = 5, perc.under = 2400)
y_train <- as.numeric(balanced.data2$target) %>% as.matrix()
y_test <- as.numeric(xtest$target) %>% as.matrix()
x_train <- balanced.data2 %>% select(-c(target)) %>% as.matrix()
x_test <- xtest %>% select(-c(target)) %>% as.matrix()
#defining a neural net model
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'tanh', input_shape = c(20)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 256, kernel_initializer = "uniform", activation = "tanh") %>%
layer_dropout(0.1) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'softmax')
model_nn %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
#selecting columns which turns out to be important in random forest
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count","most_presc_drug_TRAMADOL",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
as.data.frame(table(balanced.data$target))
options(scipen=999)
#performing Oversampling to generate synthetic data
set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
as.data.frame(table(balanced.data$target))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
#instead of training models on all the columns, we are identifying important columns using random forest
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
fit <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
var.imp <- varImp(fit)
plot(var.imp,top=15)
#selecting columns which turns out to be important in random forest
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count","most_presc_drug_TRAMADOL",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
#splitting train and testing set (50% ratio)
smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)
xtrain <- final_df[train_indices,]
xtest <- final_df[-train_indices,]
xtrain$target <- factor(xtrain$target)
xtest$target <- factor(xtest$target)
model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))
## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')
## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.4)
set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
as.data.frame(table(balanced.data$target))
#selecting columns which turns out to be important in random forest
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count","most_presc_drug_TRAMADOL",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]
#fitting our first model - logistic regression
fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 5)
model_logistic_cv <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
trControl = fitControl)
#predicted values for testdata:
pred_logistic_cv <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
#test with confusion matrix
table(pred_logistic_cv>0.4,xtest$target)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model_ridge <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
pred_ridge <- model_ridge %>% predict(newx = x.test)
table(pred_ridge>0.2,xtest$target)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
model_rf <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
pred_rf <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
table(pred_rf>0.3,xtest$target)
set.seed(123)
fitControl = trainControl(method="cv", number=10, returnResamp = "all",allowParallel = TRUE)
model_gbm = train(target~., data=balanced.data, method="gbm",distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=5000, .shrinkage=0.1, .interaction.depth=1, .n.minobsinnode=1))
pred_gbm <- predict(model_gbm,xtest,type = 'prob')
table(pred_gbm$'1'>0.2,xtest$target)
stopCluster(cluster)
registerDoSEQ()
model_ada = ada(formula = target ~ .,data=balanced.data ,iter=10)
pred_ada <- predict(model_ada,xtest,type = 'prob')
table(pred_ada[,2]>0.2,xtest$target)
# Neural net
balanced.data2 <- SMOTE(target ~., xtrain, perc.over = 4800, k = 5, perc.under = 2400)
y_train <- as.numeric(balanced.data2$target) %>% as.matrix()
y_test <- as.numeric(xtest$target) %>% as.matrix()
x_train <- balanced.data2 %>% select(-c(target)) %>% as.matrix()
x_test <- xtest %>% select(-c(target)) %>% as.matrix()
#defining a neural net model
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'tanh', input_shape = c(20)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 256, kernel_initializer = "uniform", activation = "tanh") %>%
layer_dropout(0.1) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'softmax')
model_nn %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
# Neural net
balanced.data2 <- SMOTE(target ~., xtrain, perc.over = 4800, k = 5, perc.under = 2400)
y_train <- as.numeric(balanced.data2$target) %>% as.matrix()
y_test <- as.numeric(xtest$target) %>% as.matrix()
x_train <- balanced.data2 %>% select(-c(target)) %>% as.matrix()
x_test <- xtest %>% select(-c(target)) %>% as.matrix()
#defining a neural net model
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'tanh', input_shape = c(21)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 256, kernel_initializer = "uniform", activation = "tanh") %>%
layer_dropout(0.1) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'softmax')
model_nn %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
# Neural net
balanced.data2 <- SMOTE(target ~., xtrain, perc.over = 1000, k = 5, perc.under = 500)
y_train <- as.numeric(balanced.data2$target) %>% as.matrix()
y_test <- as.numeric(xtest$target) %>% as.matrix()
x_train <- balanced.data2 %>% select(-c(target)) %>% as.matrix()
x_test <- xtest %>% select(-c(target)) %>% as.matrix()
#defining a neural net model
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'tanh', input_shape = c(21)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 256, kernel_initializer = "uniform", activation = "tanh") %>%
layer_dropout(0.1) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'softmax')
model_nn %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 30, batch_size = 128,
validation_split = 0.2
)
pred_model_nn = model_nn %>% predict(x_test)
table(pred_model_nn,xtest$target)
#overlapping all the ROC curve
roc(xtest$target,pred_logistic_cv,plot=TRUE,legacy.axes=TRUE, col="#377eb8",
xlab="False Positive Rate",ylab="True Positive Rate")
plot.roc(xtest$target,pred_ridge, col="#4daf4a",add=TRUE)
plot.roc(xtest$target,pred_rf, col="#FF420E",add=TRUE)
plot.roc(xtest$target,pred_gbm$'1', col="#FFBB00",add=TRUE)
plot.roc(xtest$target,pred_ada[,2], col="#763626",add=TRUE)
plot.roc(xtest$target,pred_model_nn, col="#375E97",add=TRUE)
legend("bottomright",legend=c("Logistic regression (LR)","Ridge LR","Random Forest","Gradient Boosting","AdaBoost","Neural Network"),
col=c("#377eb8","#4daf4a","#FF420E","#FFBB00","#763626","#375E97"),lwd=4, bg=F)
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "readxl",
"survival", "fastDummies", "DataExplorer", "ggplot2", "corrplot", "ggpubr",
"parallel", "doParallel","DMwR", "glmnet", "pROC", "keras", "tibble")
loadlibs(libs)
mode_fun <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
#--- setting global decimal print setting
options(scipen=4)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
prog = fread("program_activity.csv") %>% as_tibble()
dem = fread("demographic.csv") %>% as_tibble()
presc = fread("opiate_prescription_fills.csv") %>% as_tibble()
summary(dem %>% mutate_if(is.character, as.factor))
# dealing with missing data for RACE and GENDER
# dealing with missingness by allowing it to be its own category - because most likely MNAR
dem$RACE[which(grepl("No Data", dem$RACE))] = "No Data and Other"
# merging native hawaiian/pacific islander + no data = no data and other because the former is too small
dem$RACE[which(grepl("Native Hawaiian", dem$RACE))] = "No Data and Other"
# dealing with missingness by allowing it to be its own category - because most likely MNAR
dem$GENDER[which(grepl("No Data", dem$GENDER))] = "No Data and Other"
# merging transgendered male to female and no data = no data and other because former is too small
dem$GENDER[which(grepl("Transgendered", dem$GENDER))] = "No Data and Other"
# renaming for cleanliness and summarizing the clean demographic dataset
dem = dem %>% rename("race"=RACE, "gender"=GENDER)
summary(dem %>% mutate_if(is.character, as.factor))
#--- Loading helper files
loadlibs = function(libs) {
for(lib in libs) {
class(lib)
if(!do.call(require,as.list(lib))) {install.packages(lib)}
do.call(require,as.list(lib))
}
}
libs = c("tidyr","magrittr","purrr","dplyr","stringr","readr","data.table", "mice",
"randomForest", "ada", "gbm", "caret", "e1071", "ROCR", "ggplot2", "readxl",
"survival", "fastDummies", "DataExplorer", "ggplot2", "corrplot", "ggpubr",
"parallel", "doParallel","DMwR", "glmnet", "pROC", "keras", "tibble")
loadlibs(libs)
mode_fun <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
#--- setting global decimal print setting
options(scipen=4)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
prog = fread("program_activity.csv") %>% as_tibble()
dem = fread("demographic.csv") %>% as_tibble()
presc = fread("opiate_prescription_fills.csv") %>% as_tibble()
# summarizing the demographic dataset
summary(dem %>% mutate_if(is.character, as.factor))
# dealing with missing data for RACE and GENDER
# dealing with missingness by allowing it to be its own category - because most likely MNAR
dem$RACE[which(grepl("No Data", dem$RACE))] = "No Data and Other"
# merging native hawaiian/pacific islander + no data = no data and other because the former is too small
dem$RACE[which(grepl("Native Hawaiian", dem$RACE))] = "No Data and Other"
# dealing with missingness by allowing it to be its own category - because most likely MNAR
dem$GENDER[which(grepl("No Data", dem$GENDER))] = "No Data and Other"
# merging transgendered male to female and no data = no data and other because former is too small
dem$GENDER[which(grepl("Transgendered", dem$GENDER))] = "No Data and Other"
# renaming for cleanliness and summarizing the clean demographic dataset
dem = dem %>% rename("race"=RACE, "gender"=GENDER)
summary(dem %>% mutate_if(is.character, as.factor))
# summarizing the program data
summary(prog)
# separating overdose data into year and month
prog = prog %>% separate(OVERDOSE_DATE, c("OVERDOSE_YEAR", "OVERDOSE_MONTH"), remove=FALSE, sep=4)
# creating the outcome variable
prog$OPIATE_OVERDOSE[which(grepl(0, prog$OPIATE_OVERDOSE))] = "Non-Opiate Overdose"
prog$OPIATE_OVERDOSE[which(grepl(1, prog$OPIATE_OVERDOSE))] = "Opiate Overdose"
prog$OPIATE_OVERDOSE[which(is.na(prog$OPIATE_OVERDOSE))] = "No Overdose"
# summarizing the cleaned program dataset
summary(prog %>% mutate_if(is.character, as.factor))
# establishing the number of times an individual has used DHS programs
times_used = prog %>% group_by(PERSON_ID) %>% select(YEAR, MONTH) %>% summarise(times_used = max(sequence(n())))
# creating new predictor variables from the program dataset
prog_summary = prog %>% group_by(PERSON_ID) %>% summarise(total_cyfchild = sum(CYFCHILD),
total_cyfparent = sum(CYFPARENT),
total_mh = sum(MH),
total_da = sum(DA),
total_rx = sum(RX),
total_acj = sum(ACJ),
total_cr_cases = sum(MDJS_CR_CASES),
total_cr_drug_cases = sum(MDJS_CR_DRUG_CASES),
cohort = min(YEAR),
od_date = max(OVERDOSE_DATE),
od_year = max(OVERDOSE_YEAR),
od_month = max(OVERDOSE_MONTH),
od_type = max(OPIATE_OVERDOSE))
# summarizing the prescription dataset
summary(presc %>% mutate_if(is.character, as.factor))
# obtaining the min, max, and mean age of an individual from the prescription dataset
age_df = presc %>% group_by(PERSON_ID) %>% summarise(min_age = min(AGE), max_age = max(AGE), mean_age = round(mean(AGE)))
# if age is less 0, then take the max age else take min age,
# if max age is still less than 0 then replace with 999 (considered missing)
age_df = age_df %>% group_by(PERSON_ID) %>% mutate(age_clean = if_else(min_age<0, max_age, min_age)) %>%
mutate(age_clean = if_else(age_clean<0, 999, age_clean))
# replace ages greater than 99 with NA (considered missing)
age_df$age_clean[which(age_df$age_clean>99)] = NA
age_df$age = age_df$age_clean
# for missing ages, take the median age across the entire set of individuals
age_df$age[which(is.na(age_df$age_clean))] = median(age_df$age_clean, na.rm=TRUE)
# summarizing the cleaned age predictor variable
summary(age_df)
# extracting drug strength from label name (better version)
presc$drug_strength <- str_extract(presc$LABEL_NAME, "[0-9].*")
# drug strength indicated that these are oral solutions per 5ML, so made this change manually
presc$drug_strength[which(grepl('ORAL SOLUTION', presc$extract_strength))] <- "5-325/5ML"
# naloxone's drug strength was missing, so updated manually
presc$drug_strength[which(grepl('NALOXONE', presc$LABEL_NAME))] <- "50MG-0.5MG"
# taking values from the original drug strength column if the newly created one has missing values
presc = presc %>% mutate(drug_strength = if_else(is.na(drug_strength), DRUG_STRENGTH, drug_strength))
# creating a new column 'num_strength' that keeps only numeric values from 'drug_strength'
presc$num_strength <- str_extract(presc$drug_strength, "[0-9]*.*[0-9]")
# extracting only the opioid strength from the drug strength column
# we need this to calculate the MME
# converted the opioid strength standard format, ex: to per 1 ML in oral solution cases as opposed to 5 ML
presc <- separate(presc, num_strength, c("opioid_strength", "compound_strength"), sep = '-')
presc <- separate(presc, opioid_strength, c("opioid", "divisor"), sep = '/')
presc$opioid <- str_extract(presc$opioid, "[0-9]*.*[0-9]")
presc <- separate(presc, compound_strength, c("compound", "divisor2"), sep = '/')
presc = presc %>% mutate(divisor = if_else(is.na(divisor), divisor2, divisor))
presc = presc %>% select(-divisor2)
presc = presc %>% mutate(opioid = if_else(is.na(compound), opioid, if_else(opioid>compound, compound, opioid)))
presc$divisor[which(is.na(presc$divisor))] = 1
presc$compound[which(is.na(presc$compound))] = 1
presc = presc %>% mutate(opioid_converted = as.numeric(opioid)/as.numeric(divisor))
# creating a new column with the conversion factor associated with the drug name and dosage form
presc <- presc %>%
mutate(conversion_factor =
if_else(grepl('BUPRENORPHINE', GENERIC_NAME) & grepl('PATCH', DOSAGE_FORM_DESC), 12.6,
if_else(grepl('BUPRENORPHINE', GENERIC_NAME) & grepl('AMPUL', DOSAGE_FORM_DESC), 30,
if_else(grepl('BUTORPHANOL', GENERIC_NAME), 7,
if_else(grepl('CODEINE', GENERIC_NAME), 0.15,
if_else(grepl('DIHYDROCODEINE BITARTRATE', GENERIC_NAME), 0.25,
if_else(grepl('OPIUM', GENERIC_NAME), 1,
if_else(grepl('FENTANYL', GENERIC_NAME) & grepl('TABLET', DOSAGE_FORM_DESC), 0.13,
if_else(grepl('FENTANYL', GENERIC_NAME) & grepl('LOZENGE', DOSAGE_FORM_DESC), 0.13,
if_else(grepl('FENTANYL', GENERIC_NAME) & grepl('PATCH', DOSAGE_FORM_DESC), 7.2,
if_else(grepl('NALBUPHINE', GENERIC_NAME), 1,
if_else(grepl('FENTANYL CITRATE', GENERIC_NAME), 0.13,
if_else(grepl('HYDROCODONE', GENERIC_NAME), 1,
if_else(grepl('HYDROMORPHONE', GENERIC_NAME), 4,
if_else(grepl('LEVORPHANOL TARTRATE', GENERIC_NAME), 11,
if_else(grepl('MEPERIDINE', GENERIC_NAME), 0.1,
if_else(grepl('METHADONE', GENERIC_NAME) & opioid_converted<=20, 4,
if_else(grepl('METHADONE', GENERIC_NAME) & opioid_converted>20, 8,
if_else(grepl('MORPHINE', GENERIC_NAME), 1,
if_else(grepl('OXYCODONE', GENERIC_NAME), 1.5,
if_else(grepl('OXYMORPHONE', GENERIC_NAME), 3,
if_else(grepl('PENTAZOCINE', GENERIC_NAME), 0.37,
if_else(grepl('TAPENTADOL', GENERIC_NAME), 0.4,
if_else(grepl('TRAMADOL', GENERIC_NAME), 0.1, 0))))))))))))))))))))))))
# stripping the drug name down to common terms to lower the number of categories
presc <- presc %>%
mutate(generic_name =
if_else(grepl('DIHYDROCODEINE BITARTR', presc$GENERIC_NAME), 'DIHYDROCODEINE BITARTRATE',
if_else(grepl('BUPRENORPHINE', presc$GENERIC_NAME), 'BUPRENORPHINE',
if_else(grepl('HYDROCODONE BITARTRAT', presc$GENERIC_NAME), 'HYDROCODONE BITARTRATE',
if_else(grepl('MORPHINE SULFATE', presc$GENERIC_NAME), 'MORPHINE SULFATE',
if_else(grepl('OXYCODONE', presc$GENERIC_NAME), 'OXYCODONE',
if_else(grepl('PENTAZOCINE', presc$GENERIC_NAME), 'PENTAZOCINE',
if_else(grepl('TRAMADOL', presc$GENERIC_NAME), 'TRAMADOL',
if_else(grepl('CODEINE PHOSPHATE', presc$GENERIC_NAME), 'CODEINE PHOSPHATE',
if_else(grepl('OPIUM/BELLADONNA', presc$GENERIC_NAME), 'OPIUM/BELLADONNA',
if_else(grepl('FENTANYL', presc$GENERIC_NAME), 'FENTANYL',
if_else(grepl('HYDROMORPHONE', presc$GENERIC_NAME), 'HYDROMORPHONE',
if_else(grepl('MEPERIDINE', presc$GENERIC_NAME), 'MEPERIDINE', GENERIC_NAME
)))))))))))))
# stripping the dosage form down to common terms to lower the number of categories
presc <- presc %>%
mutate(dosage_form =
if_else(grepl('TABLET', presc$DOSAGE_FORM_DESC), 'PILL',
if_else(grepl('VIAL', presc$DOSAGE_FORM_DESC), 'VIAL',
if_else(grepl('SYRINGE', presc$DOSAGE_FORM_DESC), 'VIAL',
if_else(grepl('CAPSULE', presc$DOSAGE_FORM_DESC), 'PILL',
if_else(grepl('PATCH', presc$DOSAGE_FORM_DESC), 'PATCH',
if_else(grepl('LOZENGE', presc$DOSAGE_FORM_DESC), 'LOZENGE',
if_else(grepl('AMPUL', presc$DOSAGE_FORM_DESC), 'AMPUL',
if_else(grepl('SPRAY', presc$DOSAGE_FORM_DESC), 'SPRAY',
if_else(grepl('SOLUTION', presc$DOSAGE_FORM_DESC), 'LIQUID',
if_else(grepl('LIQUID', presc$DOSAGE_FORM_DESC), 'LIQUID',
if_else(grepl('CONCENTRATE', presc$DOSAGE_FORM_DESC), 'LIQUID',
if_else(grepl('SUPPOSITORY', presc$DOSAGE_FORM_DESC), 'SUPPOSITORY', 'NA'
)))))))))))))
# cleaning the days supply variable in the prescription dataset
loop_values = presc %>% group_by(generic_name, dosage_form) %>% summarise(count = n())
generics = loop_values[[1]]
dosages = loop_values[[2]]
min_supply_vec=rep(0, length(generics))
max_supply_vec=rep(0, length(generics))
min_dispensed_vec=rep(0, length(generics))
max_dispensed_vec=rep(0, length(generics))
# looping over each drug name and dosage type to determine the minimum and maximum days supply for that combination
for (i in 1:length(generics)){
presc_subset = presc %>% filter(generic_name==generics[i] & dosage_form==dosages[i])
min_supply_vec[i]=round(quantile(presc_subset$DAYS_SUPPLY , probs=seq(0, 1, by=0.005))[[2]])
max_supply_vec[i]=round(quantile(presc_subset$DAYS_SUPPLY, probs=seq(0, 1, by=0.005))[[200]])
min_dispensed_vec[i]=round(quantile(presc_subset$DISPENSED_QTY , probs=seq(0, 1, by=0.005))[[5]])
max_dispensed_vec[i]=round(quantile(presc_subset$DISPENSED_QTY, probs=seq(0, 1, by=0.005))[[195]])
}
min_supply_vec = min_supply_vec %>% as.data.frame()
min_supply_vec = min_supply_vec %>% rename(min_supply_val = ".")
max_supply_vec = max_supply_vec %>% as.data.frame()
max_supply_vec = max_supply_vec %>% rename(max_supply_val = ".")
min_dispensed_vec = min_dispensed_vec %>% as.data.frame()
min_dispensed_vec = min_dispensed_vec %>% rename(min_dispensed_val = ".")
max_dispensed_vec = max_dispensed_vec %>% as.data.frame()
max_dispensed_vec = max_dispensed_vec %>% rename(max_dispensed_val = ".")
values = bind_cols(loop_values, min_supply_vec)
values = bind_cols(values, max_supply_vec)
values = bind_cols(values, min_dispensed_vec)
values = bind_cols(values, max_dispensed_vec)
values = values %>% select(-count)
presc = merge(presc, values, all.x = TRUE, key=c("generic_name, dosage_form"))
# cleaning days supply variable such that it is not lower than minimum supply value or greater than maximum supply value
# for that combination of drug name and dosage form
# this helps us deal with missing or extreme days supply values
presc = presc %>% mutate(days_supply = if_else(DAYS_SUPPLY<min_supply_val, as.integer(min_supply_val),
if_else(DAYS_SUPPLY>max_supply_val, as.integer(max_supply_val),
as.integer(DAYS_SUPPLY))))
presc = presc %>% mutate(dispensed_qty = if_else(DISPENSED_QTY<min_dispensed_val,
as.integer(min_dispensed_val),
if_else(DISPENSED_QTY>max_dispensed_val,
as.integer(max_dispensed_val),
as.integer(DISPENSED_QTY))))
# creating a new variable -- mme
presc$mme <- round((presc$opioid_converted*presc$dispensed_qty*presc$conversion_factor)/presc$days_supply,2)
presc_summary = presc %>% group_by(PERSON_ID) %>% summarise(num_presc = n(),
most_presc_drug = mode_fun(generic_name),
oxy_count = sum(grepl('OXYCODONE', generic_name)),
tram_count = sum(grepl('TRAMADOL', generic_name)),
hydrobit_count = sum(grepl('HYDROCODONE BITARTRATE', generic_name)),
most_dose_form = mode_fun(dosage_form),
pill_count = sum(grepl('PILL', dosage_form)),
patch_count = sum(grepl('PATCH', dosage_form)),
liquid_count = sum(grepl('LIQUID', dosage_form)),
avg_mme = round(mean(mme)),
median_mme = round(median(mme)),
mode_mme = round(mode_fun(mme)),
avg_supply = round(mean(days_supply)),
avg_dispensed = round(mean(dispensed_qty)))
# merging cleaned demographic dataset with the newly creating features from program and prescription datasets
final_data = merge(dem, age_df %>% select(PERSON_ID, age), all.x = TRUE, all.y = TRUE, key="PERSON_ID")
final_data = merge(final_data, prog_summary, all.x = TRUE, all.y = TRUE, key="PERSON_ID")
final_data = merge(final_data, presc_summary, all.x = TRUE, all.y = TRUE, key="PERSON_ID")
# summarizing the final dataset
summary(final_data %>% mutate_if(is.character, as.factor))
