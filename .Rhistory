# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Display regression coefficients
coef(model)
x.test <- xtest %>% select(-c(target))   %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
observed.classes <- xtest$target
mean(predicted.classes == observed.classes)
library(glmnet)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Display regression coefficients
coef(model)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target))   %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
observed.classes <- xtest$target
mean(predicted.classes == observed.classes)
table(xtest$target,predicted.classes)
grid <- expand.grid(.alpha = seq(0, 1, by = 0.1), .lambda = seq(0.00, 0.3, by = 0.02))
control <- trainControl(method="cv", number=5)
enet.train <- train(target ~., data = balanced.data, method = "glmnet", trControl = control, tuneGrid = grid)
enet.y <- predict(enet.train, xtest, type = "prob")
table(xtest$target,enet.y$'1'>0.4)
stopCluster(cluster)
table(xtest$target,probabilities>0.4)
table(xtest$target,probabilities>0.3)
table(xtest$target,probabilities>0.2)
table(xtest$target,probabilities>0.1)
table(xtest$target,probabilities>0.05)
table(xtest$target,probabilities>0.005)
library(glmnet)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 0.5, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Display regression coefficients
coef(model)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target))   %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
table(xtest$target,probabilities>0.05)
library(glmnet)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
# Find the best lambda using cross-validation
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial")
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
# Display regression coefficients
coef(model)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target))   %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
rm(list=ls())
library(DataExplorer)
library(ggplot2)
library(dplyr)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
library(corrplot)
library(data.table)
final_data <- read.csv("final_data.csv")
dat <- fread("final_data.csv") %>% as_tibble()
table(final_data$od_type)
table(final_data$od_year)
#ggplot(final_data, aes(as.factor(od_type),patch_count )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
#ggplot(final_data, aes(as.factor(od_type),age )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
final_data_df <- final_data %>%
mutate(mme_group = ifelse(avg_mme %in% 1:99 ,"1.<100",
ifelse(avg_mme %in% 100:249 ,"2.100-249",
ifelse(avg_mme %in% 250:999 ,"3.250-999",
ifelse(avg_mme %in% 1000:4999 ,"4.1000-4999","5.>5000+")))))
crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data_df, row.vars = "mme_group",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "race",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "cohort",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
rm(final_data_df)
final_data <- group_category(data = final_data, feature = "most_presc_drug", threshold = 0.002, update = TRUE) %>%
group_category(feature = "most_dose_form", threshold = 0.0002, update = TRUE)
crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
#correlation matrix between numerical variables
corr_data_num <- final_data[,c(4,5,6,7,8,9,10,11,12,18,20,21,22,24,25,26,27,30,31)]
res <- cor(corr_data_num)
corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
library(ggpubr)
c1 <- ggplot(final_data, aes(avg_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c2 <- ggplot(final_data, aes(median_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c3 <- ggplot(final_data, aes(mode_mme)) + geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
ggarrange(c1,c2,c3,nrow=3,ncol=1)
rm(list=c('c1','c2','c3'))
final_data$target <- NULL
final_data$target[final_data$od_type=='No Overdose'] <- 0
final_data$target[final_data$od_type=='Non-Opiate Overdose'] <- 0
final_data$target[final_data$od_type=='Opiate Overdose'] <- 1
final_data <- final_data %>% select(-c('avg_mme','mode_mme','od_date','od_year','od_type','od_month','PERSON_ID'))
trans <- function(x){
return (log(1+x))
}
#dummifying data to create dummy categorical variables
final_df <- final_data %>% select(-c(target))
final_df <- dummify(final_df)
final_df <- final_df  %>% mutate_all(funs(trans))
final_df$target <- final_data$target
#logistic regression
smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)
xtrain <- final_df[train_indices,]
xtest <- final_df[-train_indices,]
xtrain$target <- factor(xtrain$target)
xtest$target <- factor(xtest$target)
model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))
## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')
## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.4)
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]
fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 3)
model_logistic_cv <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
trControl = fitControl)
#predicted values for testdata:
pred_balanced_logistic_cv <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
#test with confusion matrix
table(xtest$target,pred_balanced_logistic_cv>0.4)
#quantile(pred_balanced_logistic_cv, probs = seq(0, 1, by= 0.1))
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
?cv.glmnet
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.05, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.5, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.6, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.7, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = seq(0, 1, by = 0.2), family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = seq(0, 0.8, by = 0.2), family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = seq(0.1, 0.8, by = 0.1), family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
model_rf <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
pred_rf <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
table(xtest$target,pred_rf>0.2)
table(xtest$target,pred_rf>0.3)
table(xtest$target,pred_rf>0.5)
table(xtest$target,pred_rf>0.4)
pred_balanced_gbm <- predict(model_gbm,xtest,type = 'proba')
library(DataExplorer)
library(ggplot2)
library(dplyr)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
library(corrplot)
library(data.table)
final_data <- read.csv("final_data.csv")
dat <- fread("final_data.csv") %>% as_tibble()
table(final_data$od_type)
table(final_data$od_year)
#ggplot(final_data, aes(as.factor(od_type),patch_count )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
#ggplot(final_data, aes(as.factor(od_type),age )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
final_data_df <- final_data %>%
mutate(mme_group = ifelse(avg_mme %in% 1:99 ,"1.<100",
ifelse(avg_mme %in% 100:249 ,"2.100-249",
ifelse(avg_mme %in% 250:999 ,"3.250-999",
ifelse(avg_mme %in% 1000:4999 ,"4.1000-4999","5.>5000+")))))
crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data_df, row.vars = "mme_group",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "race",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "cohort",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
rm(final_data_df)
final_data <- group_category(data = final_data, feature = "most_presc_drug", threshold = 0.002, update = TRUE) %>%
group_category(feature = "most_dose_form", threshold = 0.0002, update = TRUE)
crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
#correlation matrix between numerical variables
corr_data_num <- final_data[,c(4,5,6,7,8,9,10,11,12,18,20,21,22,24,25,26,27,30,31)]
res <- cor(corr_data_num)
corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
library(ggpubr)
c1 <- ggplot(final_data, aes(avg_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c2 <- ggplot(final_data, aes(median_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c3 <- ggplot(final_data, aes(mode_mme)) + geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
ggarrange(c1,c2,c3,nrow=3,ncol=1)
rm(list=c('c1','c2','c3'))
final_data$target <- NULL
final_data$target[final_data$od_type=='No Overdose'] <- 0
final_data$target[final_data$od_type=='Non-Opiate Overdose'] <- 0
final_data$target[final_data$od_type=='Opiate Overdose'] <- 1
final_data <- final_data %>% select(-c('avg_mme','mode_mme','od_date','od_year','od_type','od_month','PERSON_ID'))
trans <- function(x){
return (log(1+x))
}
#dummifying data to create dummy categorical variables
final_df <- final_data %>% select(-c(target))
final_df <- dummify(final_df)
final_df <- final_df  %>% mutate_all(funs(trans))
final_df$target <- final_data$target
#logistic regression
smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)
xtrain <- final_df[train_indices,]
xtest <- final_df[-train_indices,]
xtrain$target <- factor(xtrain$target)
xtest$target <- factor(xtest$target)
model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))
## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')
## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.4)
library(parallel)
library(doParallel)
library(caret)
library(DMwR)
options(scipen=999)
#logistic with Over sampling
set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
#as.data.frame(table(balanced.data$target))
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
fit <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
var.imp <- varImp(fit)
var.imp
cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
"race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
"avg_supply","age","total_mh","total_acj","pill_count",
"num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")
xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]
fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 3)
model_logistic_cv <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
trControl = fitControl)
#predicted values for testdata:
pred_balanced_logistic_cv <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
#test with confusion matrix
table(xtest$target,pred_balanced_logistic_cv>0.4)
#quantile(pred_balanced_logistic_cv, probs = seq(0, 1, by= 0.1))
library(glmnet)
set.seed(123)
y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()
cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)
# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
probabilities <- model %>% predict(newx = x.test)
table(xtest$target,probabilities>0.2)
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
model_rf <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)
pred_rf <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
table(xtest$target,pred_rf>0.4)
library(gbm)
set.seed(123)
fitControl = trainControl(method="cv", number=5, returnResamp = "all",allowParallel = TRUE)
model_gbm = train(target~., data=balanced.data, method="gbm",distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=5000, .shrinkage=0.1, .interaction.depth=1, .n.minobsinnode=1))
pred_balanced_gbm <- predict(model_gbm,xtest,type = 'prob')
table(xtest$target,pred_balanced_gbm)
table(xtest$target,pred_balanced_gbm)
rm(pred_balanced_gbm)
pred_gbm <- predict(model_gbm,xtest,type = 'prob')
table(xtest$target,pred_gbm$'1'>0.4)
table(xtest$target,pred_gbm$'1'>0.2)
y_train <- balanced.data$target[train_indices] %>% as.matrix()
y_test <- balanced.data$target[-train_indices] %>% as.matrix()
x_train <- balanced.data[train_indices,] %>% select(-c(target)) %>% as.matrix()
x_test <- balanced.data[-train_indices,] %>% select(-c(target)) %>% as.matrix()
model <- keras_model_sequential()
library(keras)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(41)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
rm(model)
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(41)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model_nn %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
xtrain, ytrain,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
history <- model_nn %>% fit(
xtrain, ytrain,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(20)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model_nn %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(20)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'sigmoid')
model_nn %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
y_train <- as.numeric(balanced.data$target[train_indices]) %>% as.matrix()
history <- model_nn %>% fit(
x_train, y_train,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
y_train <- balanced.data$target[train_indices] %>% as.matrix()
y_test <- balanced.data$target[-train_indices] %>% as.matrix()
x_train <- balanced.data[train_indices,] %>% select(-c(target)) %>% as.matrix()
x_test <- balanced.data[-train_indices,] %>% select(-c(target)) %>% as.matrix()
model_nn <- keras_model_sequential()
model_nn <- model_nn %>%
layer_dense(units = 256, activation = 'relu', input_shape = c(20)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
layer_dropout(0.1) %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 1, activation = 'sigmoid')
model_nn %>% compile(
loss = 'binary_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model_nn %>% fit(
x_train, y_train,
epochs = 50, batch_size = 256,
validation_split = 0.2
)
