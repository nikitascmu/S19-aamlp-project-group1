---
title: "RostL_project_code.Rmd"
author: "Lauren Rost"
date: "4/21/2019"
output: pdf_document
---


```{r}
library(DataExplorer)
library(ggplot2)
library(dplyr)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
library(corrplot)
library(data.table)

final_data <- read.csv("final_data.csv") 
dat <- fread("final_data.csv") %>% as_tibble()

table(final_data$od_type)
table(final_data$od_year)

#ggplot(final_data, aes(as.factor(od_type),patch_count )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
ggplot(final_data, aes(as.factor(od_type),age )) + geom_point() + labs(y = "Patch count", x = "opioid overdose")
```

```{r}
final_data_df <- final_data %>% 
        mutate(mme_group = ifelse(avg_mme %in% 1:99 ,"1.<100",
                           ifelse(avg_mme %in% 100:249 ,"2.100-249", 
                           ifelse(avg_mme %in% 250:999 ,"3.250-999",
                           ifelse(avg_mme %in% 1000:4999 ,"4.1000-4999","5.>5000+")))))

crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data_df, row.vars = "mme_group",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "race",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "cohort",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)

rm(final_data_df)
```

#some drugs and dosage form counts are very less so there is no need to keep them as a separate category. I'm going to create a separate category to "Other" in which I'll combine all such categories.

```{r}
final_data <- group_category(data = final_data, feature = "most_presc_drug", threshold = 0.002, update = TRUE) %>%
            group_category(feature = "most_dose_form", threshold = 0.0002, update = TRUE)

crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
```

```{r pressure, echo=FALSE}
#correlation matrix between numerical variables
corr_data_num <- final_data[,c(4,5,6,7,8,9,10,11,12,18,20,21,22,24,25,26,27,30,31)]
res <- cor(corr_data_num)
corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

```{r}
library(ggpubr)
c1 <- ggplot(final_data, aes(avg_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c2 <- ggplot(final_data, aes(median_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c3 <- ggplot(final_data, aes(mode_mme)) + geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
ggarrange(c1,c2,c3,nrow=3,ncol=1)
rm(list=c('c1','c2','c3'))
```

#median_mme looks normal and less skewed. We will be using median_mme for prediction purpose. Also, I'm removing columns which are related to overdose date as this column is proxy for target variable and is not available in the future dataset.

```{r}

final_data$target <- NULL
final_data$target[final_data$od_type=='No Overdose'] <- 0
final_data$target[final_data$od_type=='Non-Opiate Overdose'] <- 0
final_data$target[final_data$od_type=='Opiate Overdose'] <- 1

final_data <- final_data %>% select(-c('avg_mme','mode_mme','od_date','od_year','od_type','od_month','PERSON_ID'))

#dummifying data to create dummy categorical variables
final_df <- dummify(final_data)
```



# Maybe we should treat instance as a x to train on, and not consolidate all people into one line. Might be more informative to time data.

# Using od_type as outcome 
# could also use total_da


# For survival analysis,
# should we do the number of years since being admitted in the system? to allow us to look at more than just the 2009 cohort?

# use balanced.data - for training purposes, test data from data_df
# final_df$target <- opiate overdose vs. the rest
```{r}
# Multivariate logistic regression

# 50-50 split
dat_scrambled <- dat[sample(1:nrow(dat)),]
dat_train <- dat_scrambled[1:(dim(dat_scrambled)[1]/2),]
dat_test <- dat_scrambled[-(1:(dim(dat_scrambled)[1]/2)),]

dat_binary_odtype <- dat
dat_binary_odtype$od_type[which(dat_binary_odtype$od_type=="No Overdose")] <- 0
dat_binary_odtype$od_type[which(dat_binary_odtype$od_type=="Non-Opiate Overdose")] <- 0
dat_binary_odtype$od_type[which(dat_binary_odtype$od_type=="Opiate Overdose")] <- 1


# Predicting "Opiate Overdose" vs. no overdose or non-opiate overdose based on total_cr_drug_cases
lr = with(dat_train, glm(od_type=="Opiate Overdose" ~ total_cr_drug_cases, family = binomial("logit")))  
lr %>% summary()    # estimates are the data coefficients; intercept = B0; each after is a B for the dummy variable; stars give the level of significance of the coefficients; all values are compared to a reference value (b was the reference value here); "values" can also be called "indicator values"; extract the Beta-hats
lr %>% str()
lr %>% summary() %>% coef()
predictions = data.frame(preds=(lr %>% predict(dat_test, type="response"))) # get predictions on new data; "response" gives predicted probabilities
library(ggplot2)
dat_test %>%
  select(od_type) %>%
  bind_cols(predictions) %>%
  table() %>%
  data.frame() %>% as_tibble() %>%
  group_by(preds) %>%
  mutate(testfreq = sum(Freq)) %>%
  mutate(testfreq = Freq/testfreq) %>%
  ungroup() %>% filter(od_type=='Opiate Overdose') %>%
  select(preds,testfreq) %>%   # this model is relatively well-calibrated; if predictions perfectly calibrated, y=x
  mutate_all(as.character) %>%
  mutate_all(as.numeric) %>%
  ggplot(data=., aes(x=preds, y=testfreq)) + geom_point() + geom_abline(slope=1)

```

```{r}
library(ROCR)
columns_for_ROC =  # columns: predictions, labels
  dat_test %>%
  select(od_type) %>%
  bind_cols(predictions) %>% mutate(od_type=od_type=="Opiate Overdose")   # true labels, and their predictions

# plot it:
prediction(predictions=columns_for_ROC$preds,
           labels=columns_for_ROC$od_type) %>%
  performance("tpr", "fpr") %>% plot()
  # performance("auc")  # swap out to get AUC

```

```{r}
# Regularized multivariate logistic regression (lasso)
# Jumping-off point from Dr. Weiss' example to work off of:

# library(glmnet)
# library(tidyr)
# mushmm = model.matrix(type~., mush[,-17]) # turns categorical vars into indicators
# mushmmtrain = mushmm[1:5000,]
# mushmmtest = mushmm[-(1:5000),]
# lr1 = glmnet(x = mushmmtrain, y=mushtrain$type, family="binomial")
# str(lr1)
# which(coef(lr1)[,20] > 0)
# which(coef(lr1)[,10] > 0)
# 
# data.frame(preds = lr1 %>% predict(mushmmtest, s=0.2, type="response"),
#            y = mushtest$type) %>% tbl_df() %>%
#   table() %>% data.frame() %>% tbl_df() %>%
#   transmute(type=y, preds=X1, Freq=Freq)%>%
#   arrange(preds) %>%
#   group_by(preds) %>%
#   mutate(testfreq = sum(Freq)) %>%
#   mutate(testfreq = Freq/testfreq) %>% 
#   ungroup() %>%
#   filter(type=='p') %>%
#   select(preds,testfreq)


```


```{r}
# Random forest
library(randomForest)
dat_scrambled <- as_tibble(dat_scrambled)

# NAs introduced by coercion; col 14, 15, 16 (119428)

data_fac <- dat_binary_odtype %>% mutate_if(is.character, as.factor)

forest = randomForest(formula = as.factor(data_fac$od_type) ~ .,
                      data=data_fac[, -c(14:16)] %>% filter(!is.na(od_type)))
forest$importance
library(ggplot2)
ggplot(data=forest$importance %>% as.data.frame() %>%
         mutate(name=as.factor(rownames(.))) %>%
         arrange(MeanDecreaseGini) %>%
         mutate(name=factor(name, name))) +
  geom_bar(aes(x=name, y=MeanDecreaseGini), stat="identity") + coord_flip()
predict(forest, data_fac, type = "prob") %>% hist(breaks=100) # %>% head()

```


```{r}
# Boosting
library(ada)
aforest = ada(formula = od_type ~ .,
              data=data_fac[-c(14:16)] %>% filter(!is.na(od_type)),
              iter=10)
predict(aforest, data_fac[,-c(14:16)],
        type = "probs") %>% hist(breaks=100) # %>% head()

```

```{r}
# Gradient boosting machine
library(gbm)
gforest = gbm(formula = od_type==1 ~ .,
              data=data_fac %>% filter(!is.na(od_type)) %>%
                mutate_if(is.logical, as.factor),
              interaction.depth=10,
              cv.folds = 2)
predict(gforest, data_fac %>% mutate_if(is.logical, as.factor),
        n.trees=gforest$n.trees,
        type = "response") %>% hist(breaks=100)  # %>% head()

```

```{r}
library(keras)
# Neural net
trans <- function(x){
  return (log(1+x))
}

#performing a 50%/50% split
smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)


ytrain <- to_categorical(final_df$target[train_indices]) %>% as.matrix()
ytest <- to_categorical(final_df$target[-train_indices]) %>% as.matrix()

xtrain <- final_df[train_indices,] %>% select(-c(target)) %>% as.matrix()
xtest <- final_df[-train_indices,] %>% select(-c(target)) %>% as.matrix()

model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(41)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense( units= 16, kernel_initializer = "uniform", activation = "relu") %>%
  layer_dropout(0.1) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'softmax')

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history <- model %>% fit(
  xtrain, ytrain, 
  epochs = 30, batch_size = 256, 
  validation_split = 0.2
)

pred_model = model %>% predict(xtest)
library(pROC)
roc(ytest,pred_model,plot=TRUE,legacy.axes=TRUE, col="#377eb8",print.auc=TRUE,percent = TRUE,print.auc.y=60,
    xlab="false positive percentage",ylab="true positive percentage")

``` 

#balancing dataset

```{r}

#logistic regression

smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)
xtrain <- final_df[train_indices,]
xtest <- final_df[-train_indices,] 

xtrain$target <- factor(xtrain$target)
xtest$target <- factor(xtest$target)

model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))

## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')

## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.5)
```

#Not able to capture a single opioid overdose case with logistic regression. Next we are trying SMOTE (Synthetic Minority Oversampling Technique)

```{r}
library(DMwR)

#logistic with sampling

set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 1500, k = 5, perc.under = 400)
as.data.frame(table(balanced.data$target))

model_logistic_balanced <- glm (target~.-race_White -gender_No.Data.and.Other -most_presc_drug_TRAMADOL-most_dose_form_PILL, 
                                data=balanced.data, family = binomial,control = list(maxit = 50))

## Predict the Values
pred_logistic_balanced <- predict(model_logistic_balanced, xtest, type = 'response')

## Create Confusion Matrix
table(xtest$target, pred_logistic_balanced > 0.5)
```

#here we are capturing atleast 79 opioid overdose. Logistic regression CV:

```{r}
library(caret)

#Cross Validation Logistic regression

balanced.data <- balanced.data %>% select(-c('race_White','gender_No.Data.and.Other','most_presc_drug_TRAMADOL','most_dose_form_PILL'))

fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 3)

caret.logreg.fit <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
                          trControl = fitControl)


#predicted values for testdata:
pred_balanced_logistic_cv <- predict(caret.logreg.fit$finalModel,xtest,type = 'response')

#test with confusion matrix
table(pred_balanced_logistic_cv>0.5,xtest$target)

```

#Results are same as the normal logistic regression 

```{r}
library(e1071)

tune <- tune.svm(target~., data = balanced.data, gamma =seq(.02, 0.1, by = .02), cost = seq(0.2,1, by = 0.2))


```

