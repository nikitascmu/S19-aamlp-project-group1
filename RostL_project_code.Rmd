---
title: "RostL_project_code.Rmd"
author: "Lauren Rost""Mridul Gangwar"
date: "4/21/2019"
output: pdf_document
---


```{r}
library(DataExplorer)
library(ggplot2)
library(dplyr)
source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")
library(corrplot)
library(data.table)

final_data <- read.csv("final_data.csv") 
dat <- fread("final_data.csv") %>% as_tibble()

table(final_data$od_type)

final_data_df <- final_data %>% 
        mutate(mme_group = ifelse(avg_mme %in% 1:99 ,"1.<100",
                           ifelse(avg_mme %in% 100:249 ,"2.100-249", 
                           ifelse(avg_mme %in% 250:999 ,"3.250-999",
                           ifelse(avg_mme %in% 1000:4999 ,"4.1000-4999","5.>5000+")))))

crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data_df, row.vars = "mme_group",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "race",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "cohort",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)

rm(final_data_df)
```

#some drugs and dosage form counts are very less so there is no need to keep them as a separate category. I'm going to create a separate category to "Other" in which I'll combine all such categories.

```{r}
final_data <- group_category(data = final_data, feature = "most_presc_drug", threshold = 0.002, update = TRUE) %>%
            group_category(feature = "most_dose_form", threshold = 0.0002, update = TRUE)

crosstab(final_data, row.vars = "most_presc_drug",col.vars = "od_type", type = "f", addmargins = FALSE)
crosstab(final_data, row.vars = "most_dose_form",col.vars = "od_type", type = "f", addmargins = FALSE)
```

```{r pressure, echo=FALSE}
#correlation matrix between numerical variables
corr_data_num <- final_data[,c(4,5,6,7,8,9,10,11,12,18,20,21,22,24,25,26,27,30,31)]
res <- cor(corr_data_num)
corrplot(res, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

```{r}
library(ggpubr)
c1 <- ggplot(final_data, aes(avg_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c2 <- ggplot(final_data, aes(median_mme)) +geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
c3 <- ggplot(final_data, aes(mode_mme)) + geom_histogram(fill="#336B87",breaks=c(seq(100,6000, by=150)))
ggarrange(c1,c2,c3,nrow=3,ncol=1)
rm(list=c('c1','c2','c3'))
```

#median_mme looks normal and less skewed. We will be using median_mme for prediction purpose. Also, I'm removing columns which are related to overdose date as this column is proxy for target variable and is not available in the future dataset.

```{r}

final_data$target <- NULL
final_data$target[final_data$od_type=='No Overdose'] <- 0
final_data$target[final_data$od_type=='Non-Opiate Overdose'] <- 0
final_data$target[final_data$od_type=='Opiate Overdose'] <- 1

final_data <- final_data %>% select(-c('avg_mme','mode_mme','od_date','od_year','od_type','od_month','PERSON_ID'))

trans <- function(x){
  return (log(1+x))
}


#dummifying data to create dummy categorical variables
final_df <- final_data %>% select(-c(target))
final_df <- dummify(final_df)
final_df <- final_df  %>% mutate_all(funs(trans))
final_df$target <- final_data$target

```


# Multivariate Logistic Regression

```{r}

#logistic regression

smp_size <- floor(0.5 * nrow(final_df))
set.seed(1)
train_indices <- sample(seq_len(nrow(final_df)),size=smp_size)

xtrain <- final_df[train_indices,]
xtest <- final_df[-train_indices,] 

xtrain$target <- factor(xtrain$target)
xtest$target <- factor(xtest$target)

model_logistic <- glm (target~., data=xtrain, family = binomial,control = list(maxit = 50))

## Predict the Values
predict_logistic <- predict(model_logistic, xtest, type = 'response')

## Create Confusion Matrix
table(xtest$target, predict_logistic > 0.4)


```

#Not able to capture a single opioid overdose case with logistic regression. Next we are trying SMOTE (Synthetic Minority Oversampling Technique)

# identifying important variables using random forest

```{r}
library(parallel)
library(doParallel)
library(caret)
library(DMwR)
options(scipen=999)

#logistic with Over sampling

set.seed(1)
balanced.data <- SMOTE(target ~., xtrain, perc.over = 500, k = 5, perc.under = 500)
as.data.frame(table(balanced.data$target))

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

fit <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)

var.imp <- varImp(fit)
plot(var.imp,top=15)
```

#running logistic regression using on selected features

```{r}

cols_to_select = c("tram_count","total_da","race_White","total_cr_drug_cases","median_mme","total_cr_cases",
                   "race_Black.African.American","avg_dispensed","most_presc_drug_OXYMORPHONE.HCL","gender_Male","gender_Female",
                   "avg_supply","age","total_mh","total_acj","pill_count",
                   "num_presc","hydrobit_count","oxy_count","most_presc_drug_HYDROCODONE.BITARTRATE","target")

xtrain <- xtrain[,c(cols_to_select)]
xtest <- xtest[,c(cols_to_select)]
balanced.data <- balanced.data[,c(cols_to_select)]


fitControl <- trainControl(method = "repeatedcv",number = 5,repeats = 3)

model_logistic_cv <- train(target ~ ., data = balanced.data, method = "glm",family = binomial(link = "logit"),
                          trControl = fitControl)


#predicted values for testdata:
pred_logistic_cv <- predict(model_logistic_cv$finalModel,xtest,type = 'response')

#test with confusion matrix
table(pred_logistic_cv>0.4,xtest$target)

#quantile(pred_balanced_logistic_cv, probs = seq(0, 1, by= 0.1))
```

# ridge regression

```{r}
library(glmnet)
set.seed(123) 

y = balanced.data$target %>% as.matrix()
x = balanced.data %>% select(-c(target))   %>% as.matrix()

cv.lasso <- cv.glmnet(x, y, alpha = 0.1, family = "binomial",nfolds=10,type.measure = "auc")
model_ridge <- glmnet(x, y, alpha = 1, family = "binomial",lambda = cv.lasso$lambda.min)

# Make predictions on the test data
x.test <- xtest %>% select(-c(target)) %>% as.matrix()
pred_ridge <- model_ridge %>% predict(newx = x.test)

table(pred_ridge>0.2,xtest$target)
```

#random forest

```{r}

fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

model_rf <- train(target ~ ., method="rf",data=balanced.data,trControl = fitControl)

pred_rf <- predict(model_logistic_cv$finalModel,xtest,type = 'response')
table(pred_rf>0.3,xtest$target)
```


#gbm

```{r}
library(gbm)

set.seed(123)
fitControl = trainControl(method="cv", number=5, returnResamp = "all",allowParallel = TRUE)

model_gbm = train(target~., data=balanced.data, method="gbm",distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=5000, .shrinkage=0.1, .interaction.depth=1, .n.minobsinnode=1))

pred_gbm <- predict(model_gbm,xtest,type = 'prob')

table(pred_gbm$'1'>0.2,xtest$target)

stopCluster(cluster)
registerDoSEQ()
```


```{r}
# Boosting
library(ada)

model_ada = ada(formula = target ~ .,data=balanced.data ,iter=10)

pred_ada <- predict(model_ada,xtest,type = 'prob')

table(pred_ada[,2]>0.2,xtest$target)
```


```{r}
library(keras)
# Neural net

balanced.data2 <- SMOTE(target ~., xtrain, perc.over = 4800, k = 5, perc.under = 2400)


y_train <- as.numeric(balanced.data2$target) %>% as.matrix()
y_test <- as.numeric(xtest$target) %>% as.matrix()

x_train <- balanced.data2 %>% select(-c(target)) %>% as.matrix()
x_test <- xtest %>% select(-c(target)) %>% as.matrix()

model_nn <- keras_model_sequential() 
model_nn <- model_nn %>% 
  layer_dense(units = 256, activation = 'tanh', input_shape = c(20)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense( units= 256, kernel_initializer = "uniform", activation = "tanh") %>%
  layer_dropout(0.1) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, activation = 'softmax')

model_nn %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history <- model_nn %>% fit(
  x_train, y_train, 
  epochs = 50, batch_size = 128, 
  validation_split = 0.2
)

pred_model_nn = model_nn %>% predict(x_test)
table(pred_model_nn,xtest$target)

``` 

```{r}
library(pROC)

roc(xtest$target,pred_logistic_cv,plot=TRUE,legacy.axes=TRUE, col="#377eb8",print.auc=TRUE,percent = TRUE,print.auc.y=60,
    xlab="false positive percentage",ylab="true positive percentage")
plot.roc(xtest$target,pred_ridge, col="#4daf4a",print.auc=TRUE,add=TRUE,percent=TRUE,print.auc.y=50)
plot.roc(xtest$target,pred_rf, col="#FF420E",print.auc=TRUE,add=TRUE,percent=TRUE,print.auc.y=40)
plot.roc(xtest$target,pred_gbm$'1', col="#FFBB00",print.auc=TRUE,add=TRUE,percent=TRUE,print.auc.y=30)
plot.roc(xtest$target,pred_ada[,2], col="#763626",print.auc=TRUE,add=TRUE,percent=TRUE,print.auc.y=20)
plot.roc(xtest$target,pred_model_nn, col="#375E97",print.auc=TRUE,add=TRUE,percent=TRUE,print.auc.y=10)

legend("bottomright",legend=c("Logistic regressio (LR)","Ridge LR","Random Forest","Gradient Boosting","Ada Boost","Neural Net"),
            col=c("#377eb8","#4daf4a","#FF420E","#FFBB00","#763626","#375E97"),lwd=4)

```




